

problem:
1. k=1 always have the smallest errors. 
2. couldn't always find the actual number of cluster
3. what to modify? thinking about modify ms_e

```{r}
house.votes <- read.csv("~/Desktop/research/research/house-votes-84.data", header=FALSE, na.strings="?",stringsAsFactors =TRUE)
encode<-function(x){
  factor(x,levels = c('n', 'y'),labels = c(1, 2))%>%as.numeric()
}
#x variables
house_votes<-apply(house.votes[2:17],2,encode)
#y var
votes_y<-factor(house.votes[,1])%>%as.numeric()

Train_house <- gen_train_test(house_votes,78)$train
Valid_house <- gen_train_test(house_votes,78)$test
k<-5
mse_df <- tibble(k=1:5)
for (k in 1:5) {
  initc <- gen_initC(Train_house,k)
partial_model <- Partial_km(Train_house,k,initc,100)
partial_test <- fitted.test(Valid_house,k,partial_model$fitted_Centroid)
mse_df$error[k] <- partial_test$ms_e
}
ggplot(data = mse_df, aes(x = k, y = error)) +
  geom_point()+geom_line()
mse_df

# initc <- gen_initC(Train_house,k)
# partial_model <- Partial_km(Train_house,k,initc,100)
# partial_test <- fitted.test(Valid_house,k,partial_model$fitted_Centroid)
# partial_test$ms_e
# test_accuracy(,partial_test$fitted_values)
# 
# hist(partial_test$fitted_values)
# hist(votes_y[-trainIndex])
# as.data.frame(partial_model$fitted_value)
# Train_house
# imput_model <- ClustImpute(Train_house,nr_cluster=3)
# impute_test <- fitted.test(Valid_house,k,imput_model$centroids[,-17])
# impute_cluster <- factor(impute_test$fitted_values,labels = c(2,1))
# test_accuracy(votes_y[-trainIndex],impute_cluster)

```

except k=1, k=2 has the smallest error. From the original dataset we know k=2 is correct   

```{r}
penguins[,c(1:2,7:8)]<-apply(penguins[,c(1:2,7:8)],2,function(x){as.numeric(as.factor(x))})
penguins_train <- gen_train_test(penguins[,-1],seed=78)$train
penguins_test <- gen_train_test(penguins[,-1],seed=78)$test
k<-5
mse_df <- tibble(k=1:5)
for (k in 1:5) {
  initc <- gen_initC(penguins_train,k)
partial_model <- Partial_km(penguins_train,k,initc,100)
partial_test <- fitted.test(penguins_test,k,partial_model$fitted_Centroid)
mse_df$error[k] <- partial_test$ms_e
}
ggplot(data = mse_df, aes(x = k, y = error)) +
  geom_point()+geom_line()
mse_df
```


## examples of dataset with m% missing values

### 10% missing
```{r}
library(mlbench)
library(dplyr)
library(ClusterR)
library(caret)
library(ClustImpute)
data("BreastCancer")
BC <- BreastCancer %>% select(-"Class",-"Id")
BC$Class <- factor(BreastCancer$Class,labels =c(2,1)) %>% as.numeric()
BC <-  apply(BC,2,function(x){as.numeric(x)}) %>% as.data.frame()
BC_10 <- gen_missing_val(BC,34,0.1)
train_BC <- gen_train_test(BC_10,34)$train
test_BC <- gen_train_test(BC_10,34)$test


mse_df10 <- tibble(k=1:5)
for (k in 1:5) {
  initc <- gen_initC(train_BC[,-10],k)
partial_model <- Partial_km(train_BC[,-10],k,initc,100)
partial_test <- fitted.test(test_BC[,-10],k,partial_model$fitted_Centroid)
mse_df10$error[k] <- partial_test$ms_e
}
ggplot(data = mse_df10, aes(x = k, y = error)) +
  geom_point()+geom_line()
mse_df10
```

k error
1	59.95692			
2	110.71835			
3	101.28885			
4	113.30574			
5	129.39373





### 30% missing
```{r}
BC_30 <- gen_missing_val(BC,34,0.3)
train_BC <- gen_train_test(BC_30,34)$train
test_BC <- gen_train_test(BC_30,34)$test

mse_df <- tibble(k=1:5)
for (k in 1:5) {
  initc <- gen_initC(train_BC[,-10],k)
partial_model <- Partial_km(train_BC[,-10],k,initc,100)
partial_test <- fitted.test(test_BC[,-10],k,partial_model$fitted_Centroid)
mse_df$error[k] <- partial_test$ms_e
}
ggplot(data = mse_df, aes(x = k, y = error)) +
  geom_point()+geom_line()
mse_df
```
k error
1	44.88216			
2	85.08033			
3	92.32898			
4	86.61729			
5	107.28461			

### 20% missing
```{r}
BC_30 <- gen_missing_val(BC,34,0.2)
train_BC <- gen_train_test(BC_30,34)$train
test_BC <- gen_train_test(BC_30,34)$test

mse_df <- tibble(k=1:5)
for (k in 1:5) {
  initc <- gen_initC(train_BC[,-10],k)
partial_model <- Partial_km(train_BC[,-10],k,initc,100)
partial_test <- fitted.test(test_BC[,-10],k,partial_model$fitted_Centroid)
mse_df$error[k] <- partial_test$ms_e
}
ggplot(data = mse_df, aes(x = k, y = error)) +
  geom_point()+geom_line()
mse_df
```
k error
1	44.88216			
2	85.08033			
3	92.32898			
4	86.61729			
5	107.28461	

```{r}
BC_50 <- gen_missing_val(BC,34,0.5)
apply(BC_50[,-10],1,function(x){all(is.na(x)==TRUE)})%>%which()
BC_50 <- BC_50[c(-691),]
train_BC <- gen_train_test(BC_50,34)$train

test_BC <- gen_train_test(BC_50,34)$test


mse_df <- tibble(k=1:5)
for (k in 1:5) {
  initc <- gen_initC(train_BC[,-10],k)
partial_model <- Partial_km(train_BC[,-10],k,initc,100)
partial_test <- fitted.test(test_BC[,-10],k,partial_model$fitted_Centroid)
mse_df$error[k] <- partial_test$ms_e
}
ggplot(data = mse_df, aes(x = k, y = error)) +
  geom_point()+geom_line()
mse_df
```

### 40% missing
```{r}
BC_40 <- gen_missing_val(BC,34,0.4)
train_BC <- gen_train_test(BC_40,34)$train
test_BC <- gen_train_test(BC_40,34)$test
model40 <- optimal_k(5,train_BC,test_BC)
model40$error_df
```

From above experiment, data with 20%-50% missing values perform better, with k=2 of second smallest error. k=1 always has the smallest error  
